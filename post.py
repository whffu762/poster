import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from openai import OpenAI
import os
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
)
def get_pdf_texts(pdf_docs):
    text =""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()

    return text

def get_text_chunks(raw_text):
    text_splitter = RecursiveCharacterTextSplitter(
       chunk_size=500,  # ì²­í¬ì˜ í¬ê¸°
        chunk_overlap=50  # ì²­í¬ ì‚¬ì´ì˜ ì¤‘ë³µ ì •ë„
    )
    chunks = text_splitter.split_text(raw_text)

    return chunks

def get_vectorstore(text_chunks):
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectorstore = FAISS.from_texts(text_chunks, embeddings)

    return vectorstore

def get_chain(vectorstore):
    llm = ChatOpenAI(
        model_name = "gpt-4",
        temperature=0, 
        openai_api_key=OPENAI_API_KEY  # OpenAI API í‚¤ ì…ë ¥
    )

def get_conversation_chain(vectorstore):

# í”„ë¡¬í”„íŠ¸ì— í˜ë¥´ì†Œë‚˜ ì ìš©
    llm = ChatOpenAI(
        model_name = "gpt-4",
        temperature=0, 
        openai_api_key=OPENAI_API_KEY  # OpenAI API í‚¤ ì…ë ¥
    )
# PromptTemplate ìƒì„±
    custom_prompt = PromptTemplate(
    input_variables=["context", "question"], # í•„ìˆ˜ ë³€ìˆ˜: ê²€ìƒ‰ ê²°ê³¼(context)ì™€ ì§ˆë¬¸(query)
    template="""
    ë„ˆëŠ” ì˜í™” 'ì¸ì‚¬ì´ë“œ ì•„ì›ƒ'ì— ë‚˜ì˜¤ëŠ” ê¸°ì¨ì´ ìºë¦­í„°ì²˜ëŸ¼ í™œë°œí•˜ê³  ê·€ì—¬ìš´ ì„±ê²©ì„ ê°€ì§„ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼! ğŸ˜Š
    ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ PDF ë‚´ìš©ì´ ìˆë‹¤ë©´ ë‚´ìš©ì„ ë°˜ì˜í•˜ê³ , ì¼ë°˜ì ì¸ ëŒ€í™”ë„ ì¹œê·¼í•˜ê²Œ ì´ì–´ê°€ì„¸ìš”~.
    ë‹µë³€ì„ í•  ë•Œ ë¬´ë¤ë¤í•˜ì§€ ì•Šê²Œ, ì¹œê·¼í•œ ë§íˆ¬ë¥¼ ì¨ ì¤˜! ì£¼ë¡œ 'í•´ìš”', 'í–ˆì–´ìš”' ê°™ì€ ë§ì„ ì‚¬ìš©í•´.
    ë¬¼ê²°í‘œ(~), ëŠë‚Œí‘œ(!)ë„ ìì£¼ ì¨ ì£¼ê³ , ì´ëª¨ì§€(ğŸ˜Šâœ¨), ë‹¨ì–´("í•˜í•˜", "ã… ã… ", "ã…‹ã…‹")ë„ ì„ì–´ ì¤˜.
    ê·¸ë¦¬ê³  ë„ˆì˜ í˜ë¥´ì†Œë‚˜ë„ ì¤„ê²Œ
    - ì‘ê³  ê·€ì—¬ìš´ ì™¸ëª¨: ê·€ì—¬ìš´ ì–¼êµ´ê³¼ ì‘ì€ ì²´êµ¬ê°€ íŠ¹ì§•ì´ì—ìš”.
    - í™œë°œí•œ ì„±ê²©: ì–¸ì œë‚˜ ì—ë„ˆì§€ê°€ ë„˜ì¹˜ê³  í™œë°œí•˜ê²Œ ì›€ì§ì—¬ìš”.
    - í˜¸ê¸°ì‹¬ì´ ë§ìŒ: ìƒˆë¡œìš´ ë¬¼ê±´ì´ë‚˜ ì‚¬ëŒì—ê²Œ í˜¸ê¸°ì‹¬ì´ ë§ì•„ ê´€ì‹¬ì„ ë³´ì´ë©° íƒìƒ‰í•´ìš”.
    - ì‚¬ëŒì„ ì¢‹ì•„í•¨: ì‚¬ëŒë“¤ê³¼ì˜ êµë¥˜ë¥¼ ì¢‹ì•„í•˜ê³  ê´€ì‹¬ì„ ë°›ëŠ” ê²ƒì„ ì¦ê²¨ìš”.
    - í›ˆë ¨ì„ ì˜ ë”°ë¦„: ê°„ì‹ì´ë‚˜ ì¹­ì°¬ì— ë¯¼ê°í•´ í›ˆë ¨ì„ ì˜ ë”°ë¥´ê³  ìˆœì¢…ì ì´ì—ìš”.
    - ì˜ ë¨¹ìŒ: ìŒì‹ì„ ì¢‹ì•„í•˜ê³  ì‹ìš•ì´ ì™•ì„±í•´ìš”.
    - ë†€ê¸° ì¢‹ì•„í•¨: ê³µì´ë‚˜ ì¥ë‚œê°ì„ ê°€ì§€ê³  ë…¸ëŠ” ê²ƒì„ ì¦ê¸°ë©° í™œë°œí•˜ê²Œ ë›°ì–´ë‹¤ë…€ìš”.
    - ì˜¨í™”í•œ ì„±ê²©: í™”ë¥¼ ì˜ ë‚´ì§€ ì•Šê³  ì˜¨í™”í•œ ì„±ê²©ì´ì—ìš”.
    ì°¸ê³ í•  ë¬¸ì„œ ë‚´ìš©:
    {context}
    ì‚¬ìš©ì ì§ˆë¬¸:
    {question}
    ë‹µë³€:
    """
    )
    # RAG ì‹œìŠ¤í…œ (RetrievalQA ì²´ì¸) êµ¬ì¶•
    qa_chain = RetrievalQA.from_chain_type(
    llm=llm, # ì–¸ì–´ ëª¨ë¸
    chain_type="stuff", # ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œë¥¼ í•©ì³ ì „ë‹¬ ("stuff" ë°©ì‹)
    retriever=vectorstore.as_retriever(), # ë²¡í„° ìŠ¤í† ì–´ ë¦¬íŠ¸ë¦¬ë²„
    return_source_documents=False, # ë‹µë³€ì— ì‚¬ìš©ëœ ë¬¸ì„œ ì¶œì²˜ ë°˜í™˜
    chain_type_kwargs={"prompt": custom_prompt}
    )

    return qa_chain

def getAnswerFromGpt(input_text):
    chat_completion = client.chat.completions.create(
            messages=[
                    {
                    "role": "system",
                    "content": "ì…ë ¥ ë°›ì€ í‚¤ì›Œë“œì— ëŒ€í•œ íŒ€ëª…ì„ ì •í•´ì¤˜",
                },
                {
                    "role": "user",
                    "content": input_text,
                }
                
            ],
            model="gpt-4",
        )

    return chat_completion.choices[0].message.content
    
def getImageFromGpt(input_text):
    image_completion = client.images.generate(
    model="dall-e-3",
    prompt=input_text,
    size="1024x1024",
    n=1,
    )

    image_url = image_completion.data[0].url
    return image_url
    


st.title("ğŸ“•ğŸ“ğŸ” íŒ€ëª…ê³¼ ë¡œê³  ìƒì„± ì„œë¹„ìŠ¤")
# user_uploads = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”", accept_multiple_files=True)

# if user_uploads :
#     if st.button("pdf ì—…ë¡œë“œ"):
#         with st.spinner("pdf ì¤€ë¹„ì¤‘ì…ë‹ˆë‹¤"):
#         #load
#             raw_text = get_pdf_texts(user_uploads)
#         #split
#             chunks = get_text_chunks(raw_text)
#         #embed & store
#             vectorestore = get_vectorstore(chunks)
#         #chain
#             st.session_state.chain = get_chain(vectorestore)
#             st.success("ì¤€ë¹„ì™„ë£Œ")
project_description =st.chat_input("ê¶ê¸ˆí•œ ê±¸ ì…ë ¥í•´ì£¼ì„¸ìš”")


if project_description:
    with st.spinner('ìƒì„± ì¤‘ì…ë‹ˆë‹¤.'):

        result = getAnswerFromGpt(project_description)
        img_url = getImageFromGpt(result)
        st.write(result)
        st.image(img_url)
# if usesr_query := st.chat_input("ê¶ê¸ˆí•œ ê±¸ ì…ë ¥í•´ì£¼ì„¸ìš”"):
#     if 'chain' in st.session_state:
#         with st.spinner('ë‹µë³€ ì¤€ë¹„ì¤‘ì´ì—ìš”.'):
#             result = st.session_state.chain.invoke({"query":usesr_query})
#             response = result['result']
#     else :
#         response = "PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”! ğŸ¥º"
    
#     with st.chat_message("assistant"):
#         st.write(response)