import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from openai import OpenAI
import os

OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
)

def get_pdf_texts(pdf_docs):
    text =""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()

    return text

def get_text_chunks(raw_text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50
    )
    chunks = text_splitter.split_text(raw_text)

    return chunks

def get_vectorstore(text_chunks):
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectorstore = FAISS.from_texts(text_chunks, embeddings)

    return vectorstore


def get_conversation_chain(vectorstore):

    # í”„ë¡¬í”„íŠ¸ì— í˜ë¥´ì†Œë‚˜ ì ìš©
    llm = ChatOpenAI(
        model_name = "gpt-4",
        temperature=0, 
        openai_api_key=OPENAI_API_KEY  # OpenAI API í‚¤ ì…ë ¥
    )
    
    # PromptTemplate ìƒì„±
    custom_prompt = PromptTemplate(
        input_variables=["context", "question"], # í•„ìˆ˜ ë³€ìˆ˜: ê²€ìƒ‰ ê²°ê³¼(context)ì™€ ì§ˆë¬¸(query)
        template="""
            
        ì°¸ê³ í•  ë¬¸ì„œ ë‚´ìš©:
            {context}
        ì‚¬ìš©ì ì§ˆë¬¸:
            {question}
        ë‹µë³€:
        """
    )

    # RAG ì‹œìŠ¤í…œ (RetrievalQA ì²´ì¸) êµ¬ì¶•
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm, # ì–¸ì–´ ëª¨ë¸
        chain_type="stuff", # ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œë¥¼ í•©ì³ ì „ë‹¬ ("stuff" ë°©ì‹)
        retriever=vectorstore.as_retriever(), # ë²¡í„° ìŠ¤í† ì–´ ë¦¬íŠ¸ë¦¬ë²„
        return_source_documents=False, # ë‹µë³€ì— ì‚¬ìš©ëœ ë¬¸ì„œ ì¶œì²˜ ë°˜í™˜
        chain_type_kwargs={"prompt": custom_prompt}
    )

    return qa_chain
    
def getImageFromGpt(input_text):
    
    image_completion = client.images.generate(
        model="dall-e-3",
        prompt=input_text,
        size="1024x1024",
        n=1,
    )

    image_url = image_completion.data[0].url
    return image_url
    


st.title("ğŸ“•ğŸ“ğŸ” íŒ€ ì´ë¦„ ë° ì„¤ëª…, ë¡œê³  ìƒì„± ì„œë¹„ìŠ¤")
user_uploads = st.file_uploader("ì›í•˜ëŠ” íŒ€ ì´ë¦„ ë° ì„¤ëª… ì–‘ì‹ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”", accept_multiple_files=True)

if user_uploads :
    if st.button("ì˜ˆì‹œ PDF ì—…ë¡œë“œ"):
        with st.spinner("PDF ì¤€ë¹„ì¤‘ì…ë‹ˆë‹¤"):
            #load
            raw_text = get_pdf_texts(user_uploads)
            #split
            chunks = get_text_chunks(raw_text)
            #embed & store
            vectorestore = get_vectorstore(chunks)
            #chain
            st.session_state.converstaion = get_conversation_chain(vectorestore)            
            st.success("ì¤€ë¹„ì™„ë£Œ")


with st.form("í”„ë¡œì íŠ¸ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."):
    domain = st.text_input("í”„ë¡œì íŠ¸ ë„ë©”ì¸ì„ ì•Œë ¤ì£¼ì„¸ìš”",placeholder="ê¸ˆìœµ, ì—¬í–‰ ë“±...")
    description = st.text_area("í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì„¤ëª…ì„ í•´ì£¼ì„¸ìš”", placeholder="ì—¬í–‰ì§€ì—ì„œ ì‚¬ëŒë“¤ê³¼ ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤ì•¼")
    submitted = st.form_submit_button("ì œì¶œ")

    if submitted:
        query = 'í”„ë¡œì íŠ¸ëŠ” '+ domain +'ì— ê´€ë ¨ëœ í”„ë¡œì íŠ¸ì•¼.' + "ìì„¸í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ì•„" + description
            
        with st.spinner("ë‹µë³€ ì¤€ë¹„ ì¤‘") :
            result = st.session_state.conversation.invoke({"query" : query})
            response = result['result']
        img_url = getImageFromGpt(query + "ì´ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ëŠ” íŒ€ ë¡œê³ ë¥¼ 5ê°œ ê·¸ë ¤ì¤˜")
        st.write(response)
        st.image(img_url)